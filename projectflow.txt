Building Pipeline:
1> Create a GitHub repo and clone it in local (Add experiments).
2> Add src folder along with all components(run them individually).
3> Add data, models, reports directories to .gitignore file
4> Now git add, commit, push

Setting up dcv pipeline (without params)
5> Create dvc.yaml file and add stages to it.
6> dvc init then do "dvc repro" to test the pipeline automation. (check dvc dag)
7> Now git add, commit, push

Setting up dcv pipeline (with params)
8> add params.yaml file
9> Add the params setup (mentioned below)
10> Do "dvc repro" again to test the pipeline along with the params
11> Now git add, commit, push
`````````````
Expermients with DVC:
12> pip install dvclive  (dvclive is the extension of dvc)
13> Add the dvclive code block (mentioned below)
14> Do "dvc exp run", it will create a new dvc.yaml(if already not there) and dvclive directory (each run will be considered as an experiment by DVC)
15> Do "dvc exp show" on terminal to see the experiments or use extension on VSCode (install dvc extension)
16> Do "dvc exp remove {exp-name}" to remove exp (optional) | "dvc exp apply {exp-name}" to reproduce prev exp
17> Change params, re-run code (produce new experiments)
18> Now git add, commit, push

Adding a remote S3 storage to DVC:
19> Login to AWS console
20> Create an IAM user 
21> Create S3
22> pip install dvc[s3]
23> pip install awscli
24> aws configure
25> dev remote add -d dvcstore s3://bucketname
26> dvc commit-push the exp outcome that you want to keep
27> Finally git add, commit, push
28> We have added stages for each componenets inside dvc.yaml, dvc also offer this command "dvc stage add -n data_ingestion -d src/data_ingestion.py -o data/raw python src/data_ingestion.py" 